# 机器学习算法

# 一、概述

## 1.1 机器学习概述

1. 人工智能>机器学习>神经网络>深度学习
2. 学习方式
   - 监督学习：训练带有标签的数据
     -  回归：标签为连续型数据
     - 分类：标签为离散型数据
   - 无监督学习：训练不带有标签的数据
     - 聚类
   - 半监督学习：少量带标签的数据和大量不带标签的数据

## 1.2 数据集概述

1. 一般来说，数据可以分为三个部分：

   - 训练集：用来训练，构建模型
   - 验证集：在模型训练阶段测试模型的好坏
   - 测试集：对现有模型评估其好坏

2. 交叉验证：

   - 留出法：将数据直接划分为互斥的两类，一部分用来训练模型，一部分用来测试
   - K折交叉验证：将数据化为K等分，其中一份作为测试集，其他的作为训练集，循环多种取每一份，等到多个结果，取每个值的平均值
   - 重复K折交叉验证：重复K折交叉验证多次
   - 留一交叉验证：K折交叉验证，K=数据集个数
   - 留P交叉验证：
   - 随机排列交叉验证：

3. 留出法

   函数：train_test_split()

   ```python
   from sklearn.model_selection import train_test_split
   X_train, X_test, y_train, y_test = train_test_split(train_x, train_Y, test_size=0.4, random_state=0)	# 测试集大小为0.4
   ```

4. 交叉验证函数

   cross_val_score ()

   ```python
   scores = cross_val_score(model, train_x, train_Y, cv=5)	#cv为迭代次数
   ```

   cross_val_score 默认使用 KFold 或 StratifiedKFold 策略



## 1.3 数学概念

1. 离散度：离散度就是数据偏离平均值的程度
2. 极差：极差 = 最大值－最小值，评价一组数据的离散度的最简单方法
3. 方差：每个样本值与样本平均值之差的平方再除以样本数，反映了一组数据与其平均值的偏离程度
4. 标准差：即方差的算数平方根，反映一个数据的离散程度，或数据的波动程度
5. 协方差：衡量两个变量之间的变化方向关系，方差可以看成是协方差的一种特殊情况，即2组数据完全相同



## 1.4 特征缩放

1. 如果各个特征点之间的数据取值范围差距太大，那么在梯度下降的时候，大范围特征方向的迭代次数就会远远大于小范围方向的迭代次数，这将使得算法的收敛速度变慢，为了解决这种情况，最好将不同特征的取值定到相同或相近的范围内，那么梯度下降算法将能很快的收敛。

2. 如果某个特征的取值范围比其他特征大很多，那么数值计算（比如说计算欧式距离）就受该特征的主要支配。但实际上并不一定是这个特征最重要，通常需要把每个特征看成同等重要。归一化/标准化数据可以使不同维度的特征放在一起进行比较，可以大大提高模型的准确性。

3. 特征缩放的方法有：

   - 归一化（normalization）：归一化是将样本的特征值转换到同一量纲下，把数据映射到[0,1]或者[-1, 1]区间内
   - 标准化（standardization）：标准化是将样本的特征值转换为标准值（z值），每个样本点都对标准化产生影响。

4. 最大最小值归一化，会将样本的特征值缩放到[0,1]：
   $$
   x_i=\frac{x_i-x_{min}}{x_{max}-x_{min}}
   $$
   函数：minmax_scale()

   ```python
   from sklearn import preprocessing
   X = preprocessing.minmax_scale(X, feature_range=(0,1))
   X = sc.fit_transform(X)
   ```

   参数：

   - X：array-like，shape(n_sample, n_features)
   - feature_range：缩放后的范围，默认[0,1]
   - axis：默认为0，各个特征独立归一化，如果为1，则所有样本归一化

5. 均值归一化，将数值范围缩放到 [-1, 1] 区间里，且数据的均值变为0

$$
x_i=\frac{x_i-\overline{x}}{x_{max}-x_{min}}
$$

6. 标准化（z值归一化），将数值缩放到0附近，且数据的分布变为均值为0，标准差为1的标准正态分布：
   $$
   x_i=\frac{x_i-\overline{x}}{\sigma},\sigma为标准差
   $$
   函数：StandardScaler()

   ```python
   from sklearn.preprocessing import StandardScaler
   sc = StandardScaler()
   X = sc.fit_transform(X)
   ```

7. 稳健归一化，先减去中位数，再除以四分位间距

   函数：RobustScaler()

   ```python
   from sklearn.preprocessing import RobustScaler
   X = [[1, -2, 2],
        [-2, 1, 3],
        [100, 1, -2]]
   scaler = RobustScaler(quantile_range = (25,75))
   X = scaler.fit_transform(X)
   ```

8. 最大绝对值归一化，即将数值变为单位长度，缩放到[-1,1]，
   $$
   x_i=\frac{x}{|x_{max}|}
   $$
   函数：MaxAbsScaler()

   ```python
   from sklearn.preprocessing import MaxAbsScaler
   X = [[1,1,0,0],
        [0,2,0,1],
        [0,0,2,0]]
   scaler = MaxAbsScaler()
   X = scaler.fit_transform(X)
   ```

9. 特征缩放的适用情况：
   - 最大最小值归一化：数据分布不是正态分布或者标准差非常小，以及需要把数据固定在 [0, 1] 范围内
   - 标准化|均值归一化：需要使用距离来度量相似性的算法中，或者使用PCA技术进行降维的时候
   - 稳健归一化：数据包含太多的异常值
   - 最大绝对值归一化：针对于已经中心化的数据或稀疏数据的缩放



## 1.5 预测指标

1. 正确率（Precision）：检索出来的条目有多少是正确的

2. 召回率（Recall）：所有正确的条目有多少被检索出来了

3. F1值：2(正确率*召回率)/(正确率+召回率)

4. 数值越接近于1，效果越好

5. 关于准确率和召回率的理解：

   现有100个产品，其中有90优质产品，10个劣质产品，要求选出优质产品：

   1. 要求高准确率：从所有产品中选出所有优质产品，即从100个产品中选出90个，有10个未被选出（或召回）；即准确率100%，召回率90%
   2. 要求高召回率：所有产品都被选出，但有10个劣质产品；得准确率为90%，召回率为100%



# 二、线性回归

## 2.1 一元线性回归

1. 一元线性回归包括一个自变量和一个因变量

2. 多元线性回顾包括两个以上的自变量

3. 公式：$h_\theta(x)=\theta_0+\theta_1x$，$\theta_1$为回归线的斜率，$\theta_0$为回归线的截距

4. **正相关**：y随着x的增大而增大

5. **负相关**：y随着x的增大而减小

6. **不相关**：y随着x的增大而不变

   

## 2.2 代价函数

1. 代价函数（Cost fuction），又称损失函数，一般使用最小二乘法实现
2. 代价函数，即各个样本预测值和真实值之差的绝对值之和
3. 代价函数越大，即预测值和真实值的偏差越大
5. 相关系数：衡量线性相关性的强弱，即线性程度，用来描述两个变量之间的线性关系
6. 相关系数越大，越接近一个线性函数
7. 决定系数：用来描述非线性或有两个及以上自变量的相关关系
7. 在做回归的时候，应该不断地调整截距和斜率，使得代价函数的值最小，此时，就是最佳的线性方程，那么，如何去调整截距和斜率？
   - 梯度下降法
   - 标准方程法
8. 梯度下降法：初始化斜率和截距，求这一点的导数，根据导数找到下一个梯度下降点（根据学习率求得步长，学习率一般在[0,1]，学习率太小会导致收敛速度慢，学习率太大会导致来回迭代，找不出极小值），不断地改变截距和斜率，使得代价函数达到一个全局最小值或局部极小值
   - 缺点：得到的代价函数不一定是最优的（局部极小值）
9. 标准方程法：将代价函数转化为矩阵的形式去进行求解，最后直接得出全局最优解，但是计算量很大
   - 缺点：不适用于特征之间有共线性的数据，不适用特征列大于数据量的情况，特征越多，计算速度越慢，甚至有可能计算不出来
10. 当特征比较少的时候，可以使用标准方程法，而神经网络的基础就是梯度下降法



## 2.3 线性回归实现

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(train_x,train_Y)	# 训练模型
model.coef_		# 系数（斜率），多元线性回归有多个系数
model.intercept_	# 截距
model.predict(test_x)	# 根据模型测试数据
```

`LinearRegression`参数：

- fit_intercept：是否有截据，如果没有则直线过原点;
- normalize：是否将数据归一化;
- copy_X：默认为True，当为True时，X会被copied,否则X将会被覆写;
- n_jobs：默认值为1。计算时使用的核数



## 2.4 基函数回归

多项式回归可以通过基函数对原始数据进行变换，从而将变量间的线性回归模型转换为非线性回归模型。可以借用函数PolynomialRegression()对数据进行维度转换：

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
poly = PolynomialFeatures(2)	# 多项式的次数为2
x_poly = poly.fit_transform(train_x)
model = LinearRegression()
model.fit(train_x,train_Y)
```

`PolynomialRegression`参数：

- degree：控制多项式的次数；

- interaction_only：默认为 False，如果指定为 True，那么就不会有特征自己和自己结合的项，组合的特征中没有$a^2a^2$ 和$b^2b^2$；

- include_bias：默认为 True 。如果为 True 的话，那么结果中就会有 0 次幂项，即全为 1 这一列，如:

  $y=a_0+a_1x+a_2x^2+a_3x^3+···$	（include_bias为False）

  $y=a_0+a_1x^0+a_2x^1+a_3x^2+···$	（include_bias为True）

例如：有 aa、bb 两个特征，那么它的 2 次多项式的次数为 1,a,b,$a^2$,ab,$b^2$。

可以适当调整多项式的次数来拟合曲线。

如果设置了多项式的次数为2，那么方程为（设include_bias为False）：

```
coef = model.coef_ = [b,a]
intercept = model.intercept_ = c
y = ax^2+bx+c
```



## 2.5 岭回归

1. 正则化：如果基函数次数较高，可能会导致模型过于灵活而产生欠拟合的情况，如果对较大的模型参数进行惩罚，从而抑制模型剧烈波动，就可以解决欠拟合的问题，这种惩罚机制称为正则化

2. 岭回归（L2范数正则化），是正则化最常见的形式，处理方法为对模型系数平方和（L2范数）进行惩罚，是解决标准方程法缺点的最好方式

   ```python
   from sklearn.linear_model import RidgeCV
   X = np.linspace(0.001,1)
   model = RidgeCV(X)
   model.fit(train_x,train_Y)
   model.alpha_		# 岭系数
   model.cv_values_.shape	# (样本数量,loss系数数量)
   ```

3. Lasso（L1范数正则化）,函数：linear_model.Lasso



## 2.6 逻辑回归

1. 逻辑回归（Logistic/Sigmoid Function）是一种分类方法，有如下定义：
   $$
   g(x)=\frac{1}{1+e^{-x}}\\
   当x>0时，g(x)\in(0.5,1]，当x\leq0时，g(x)\in[0,0.5]\\
   令g(x)=g(h(x))，则\\
   h(x) = \theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\cdots \ 线性决策边界	\\	
   h(x) = \theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_2^2\cdots \ 非线性决策边界
   $$

2. 逻辑回归的代价函数一般使用梯度下降来求得，同样，他也有相应的正则化公式

3. 逻辑回归的多分类问题，其实就是将多分类转化为二分类的问题，即将某一类归于一类，其他类都归于一类来得出多个决策边界函数

4. 逻辑回归使用LogisticRegression()

   ```python
   from sklearn.linear_model import LogisticRegression
   model = LogisticRegression()
   model.fit(train_x,train_Y)
   model.coef_   		# 系数（斜率），可以理解为各个特征点的权重
   model.intercept_	# 截距
   model.predict_proba(X)	# 输出分类概率
   ```

   `LogisticRegression`参数：

   - penalty：惩罚机制，‘l1’或'l2'，默认为L2范数正则化
   - 其他详见：https://www.cnblogs.com/sddai/p/9571305.html

   

   

   